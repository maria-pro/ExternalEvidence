---
title: "External evidence"
author: "Maria Prokofieva"
date: "01/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,   
                      message = FALSE,
                      warning = FALSE,
                      fig.height = 4,
                      fig.width = 8,
                      fig.align = "center")

library(tidyverse, knitr)
```

## External evidence for audit analytics

#### Objectives

The **objectives** of the project are: 

- to examine the use of data analytics in collecting, evaluating and using audit evidence from external sources and 

- to develop guidance on knowledge and skills required for audit profession to use audit analytics.

#### Research questions

**Key research questions** to address the objectives are:

1. In what main ways can data analytics be used in the collection of audit evidence from external sources?

2. How can data analytics be used to ensure quality and verifiability of audit evidence from external sources?

3. What knowledge and skills are required for auditing professionals to successfully utilize data analytics?

#### Deliverables


### Data collection

#### Data sources stated in the proposal

The project outline includes references to the following data sources:

- Australian open government data (http://data.gov.au) 

- Social media (e.g. Twitter API https://developer.twitter.com)

- News API (https://newsapi.org/).

The particular products used for current stage of data collection:

- Twitter:
--    Company tweets
--   Tweets mentioning a company via #


The proposed dashboard employs the developed model but collect information in realtime to ensure relevance of the data. This poses limitations to the social media data and news.

#Limitations:
Twitter allows data collection of up to last 10,000 tweets and this allowance is subject to changes on their side.


The study is based on 

### Variable and measurement

### Methodology


#### Data collection

##### Companies:

The sample includes 213 companies listed at ASX. The selection was done by tracing All-Ordinary S&P/ASX index over 10 years to isolate companies who remained in the index throughout the period. 

**Rationale**: ensure consistency in data as well as representativeness of the sample. Also, it was assumed that such "stable" companies have visibility and will be presented in external data sources.

The list of companies is available [here](https://raw.githubusercontent.com/maria-pro/ExternalEvidence/master/data/listOfCompanies.csv). The file includes company names as well as identification numbers across a number of systems to allow access to company informaiton cross-platform-wise.



Sources and methods:

- [News API](https://newsapi.org/)

Data collection is automated using [News API](https://newsapi.org/). It is based on using the JSON and returns JSON metadata for headlines and articles live all over the web right now.

The API can generally be used for free for non-commercial use cases. However, some restrictions are in place: you cannot download more than 1.000 results. Upgrade is available  https://newsapi.org/pricing for commercial options. The list of available sources is available in the Appendix.

- [Google News](https://news.google.com/)

[Google News](https://news.google.com/) is a news aggregator service developed by Google. It presents a continuous flow of articles organized from thousands of publishers and magazines. 
The data was mined using R package `rvest`

- **Thomson Reuters News**

**Thomson Reuters News** is part of the university subscription to THomson Reuters. The data was collected manually using TR terminal



-----------
## Appendix

List of avalable sources for News API:

```{r}
sources<-readr::read_csv("data/APINewsSources.csv")
knitr::kable(sources[1:10,], col.names = "Sources", align="l", caption="Sample of Sources used under News API")
```

The full list is available [here](https://raw.githubusercontent.com/maria-pro/ExternalEvidence/master/data/newsAPIsources.csv)