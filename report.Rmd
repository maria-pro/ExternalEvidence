---
title: "External evidence"
author: "Maria Prokofieva"
date: "01/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,   
                      message = FALSE,
                      warning = FALSE,
                      fig.height = 4,
                      fig.width = 8,
                      fig.align = "center")

library(tidyverse, knitr)
```

## External evidence for audit analytics

The project looks into using external evidence from social media as external evidence for audit.

Social media has shown to be a valuable source of information, both from internal (i.e. the company) and external (i.e. users).

The project also looks at news, such as business press to external evidence about the company's position and performance.

The difficulty of using social media as external evidence comes from limitations of a generalizable approach that can be applied to a range of companies. Understanding possible generalization of the framework would allow reduction in time and effort of external audit.


--- Deniz' paper on audit analytics -----

Insights from **news**:

- while some companies do not have a social media account and are not tweeted about in social media, they still appear in news. 

- News are used to identify key terms for the company and the industry in general.

- 

The insights from the **social media** include:

- hashtag analysis to understand the list of hashtags associated with the company

- wordclouds and word analysis of tweets to understand "key words" associated with a particular company, range of companies or the industry. Developing a "dictionary" of such keywords is essential for developing a generalizable approach.

- sentiment analysis of tweets to ascertain mood and perception of the external users about the company

- topic modeling of tweets from external users to leverage the perception of the public vs managing expectations (from the side of a company)

### Model

The diagram below presents the broad model for the project that integrates 

- news  (external data)

- social media (external AND internal data)

- financial performance indicators (internal data)

- company announcements (internal data)


**Limitation**:

- The developed model is based on a limited set of data from ASX companies which is a smaller stock exchange. 

- It is also distorted by the recent COVID-19 events.

- The data around initial search terms and search hashtags are collected manually. Though it was cross-checked, omissions and genuin errors are possible.

Companies may have 

#### Objectives

The **objectives** of the project are: 

- to examine the use of data analytics in collecting, evaluating and using audit evidence from external sources and 

- to develop guidance on knowledge and skills required for audit profession to use audit analytics.

#### Research questions

**Key research questions** to address the objectives are:

1. In what main ways can data analytics be used in the collection of audit evidence from external sources?

2. How can data analytics be used to ensure quality and verifiability of audit evidence from external sources?

3. What knowledge and skills are required for auditing professionals to successfully utilize data analytics?

#### Deliverables

- a dashboard that allows live generation of current and recent tweets as well as analysis of tweet related information.

### Data collection

#### Data sources stated in the proposal

The project outline includes references to the following data sources:

- Australian open government data (http://data.gov.au) 

- Social media (e.g. Twitter API https://developer.twitter.com)

- News API (https://newsapi.org/).

The particular products used for current stage of data collection:

- Twitter:
--    Company tweets
--   Tweets mentioning a company via # hashtag


The proposed dashboard employs the developed model but collect information in realtime to ensure relevance of the data. This poses limitations to the social media data and news.

#Limitations:
Twitter allows data collection of up to last 10,000 tweets and this allowance is subject to changes on their side.


The study is based on 

### Variable and measurement

### Methods

At the preliminary stage the study explored the news data collected via News API. This API allows collection of only 100 entries per request. To automate this, a cron job was set up to run on a weekly basis to collect data about each company from the sample. The resulting sample was checked for duplicates and reliability by two independent researchers.

The exploratory stage started with 


#### Data collection

##### Companies:

The sample includes 213 companies listed at ASX. The selection was done by tracing All-Ordinary S&P/ASX index over 10 years to isolate companies who remained in the index throughout the period. 

**Rationale**: ensure consistency in data as well as representativeness of the sample. Also, it was assumed that such "stable" companies have visibility and will be presented in external data sources.

The list of companies is available [here](https://raw.githubusercontent.com/maria-pro/ExternalEvidence/master/data/listOfCompanies.csv). The file includes company names as well as identification numbers across a number of systems to allow access to company informaiton cross-platform-wise.



Sources and methods:

- [News API](https://newsapi.org/)

Data collection is automated using [News API](https://newsapi.org/). It is based on using the JSON and returns JSON metadata for headlines and articles live all over the web right now.

The API can generally be used for free for non-commercial use cases. However, some restrictions are in place: you cannot download more than 1.000 results. Upgrade is available  https://newsapi.org/pricing for commercial options. The list of available sources is available in the Appendix.

- [Google News](https://news.google.com/)

[Google News](https://news.google.com/) is a news aggregator service developed by Google. It presents a continuous flow of articles organized from thousands of publishers and magazines. 
The data was mined using R package `rvest`

- **Thomson Reuters News**

**Thomson Reuters News** is part of the university subscription to THomson Reuters. The data was collected manually using TR terminal



-----------
## Appendix

List of available sources for News API:

```{r}
sources<-readr::read_csv("data/APINewsSources.csv")
knitr::kable(sources[1:10,], col.names = "Sources", align="l", caption="Sample of Sources used under News API")
```

The full list is available [here](https://raw.githubusercontent.com/maria-pro/ExternalEvidence/master/data/newsAPIsources.csv)